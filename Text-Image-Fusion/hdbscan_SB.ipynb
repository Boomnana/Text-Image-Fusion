{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "from sklearn.base import BaseEstimator, ClusterMixin\n",
    "import jieba\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from sklearn.cluster import DBSCAN, AgglomerativeClustering, HDBSCAN\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score\n",
    "import spacy\n",
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "\n",
    "from docx import Document\n",
    "\n",
    "import config\n",
    "from sentence_transformers import util, SentenceTransformer\n",
    "from sklearn.metrics import adjusted_mutual_info_score, v_measure_score\n",
    "\n",
    "import openpyxl\n",
    "\n",
    "stopwords_path = './stopwords-master/hit_stopwords.txt'\n",
    "\n",
    "def cos_(ver,data_1):\n",
    "    data_1 = data_1.tolist()\n",
    "    cos_sims = []\n",
    "    ver = ver.toarray()\n",
    "    for j in range(len(ver)):\n",
    "        rows = []\n",
    "        for i in range(len(ver)):\n",
    "            sim = util.cos_sim(ver[j],ver[i])\n",
    "            rows.append(sim.tolist()[0][0])\n",
    "        cos_sims.append(rows)\n",
    "\n",
    "    #存储成Excel\n",
    "    workbook = openpyxl.Workbook()\n",
    "    worksheet = workbook.active\n",
    "\n",
    "    worksheet.append(['']+data_1)\n",
    "    for data,val in zip(data_1,cos_sims):\n",
    "        worksheet.append([data]+val)\n",
    "\n",
    "    workbook.save(config.next_feature_ext.ext+\".xlsx\")\n",
    "    workbook.close()\n",
    "\n",
    "    return sp.sparse.csr_matrix(cos_sims)\n",
    "\n",
    "def npeMethod(text):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "\n",
    "def data_post(data, cluster, file_name):\n",
    "    data['labels_'] = cluster.labels_\n",
    "\n",
    "    # 存储为json\n",
    "    json_data = {}\n",
    "    for i in range(max(cluster.labels_) + 1):\n",
    "        json_data[f\"Cluster {i + 1}\"] = list(data[data['labels_'] == i]['discription'])\n",
    "    json_data[f\"Noise:\"] = list(data[data['labels_'] == -1]['discription'])\n",
    "\n",
    "    # file_name = '' + next_feature_ext + '_' + next_cluster_me + '.json'\n",
    "    with open('./result/' + file_name+\".json\", 'w', encoding='utf-8') as f:\n",
    "        f.write(json.dumps(json_data, ensure_ascii=False, indent=2))\n",
    "    # json.dump(json_data,f)\n",
    "\n",
    "def purity_score(y_true, y_pred):\n",
    "    # 计算纯度\n",
    "    # 使用混淆矩阵（contingency matrix），其中每一列代表一个真实类别，每一行代表一个聚类\n",
    "    cont_matrix = contingency_matrix(y_true, y_pred)\n",
    "\n",
    "    # 对于每个聚类（行），找出数量最多的真实类别（列）的数量\n",
    "    # 然后将这些数量加起来\n",
    "    # 最后，将总数除以样本总数以计算纯度\n",
    "    return np.sum(np.amax(cont_matrix, axis=0)) / np.sum(cont_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\17143\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.366 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "# 读取数据\n",
    "data_path = \"JayMe标注.xlsx\"\n",
    "data = pd.read_excel(data_path, sheet_name=\"Sheet1\")  # , sep=',')\n",
    "data_ = data[\"缺陷描述\"]\n",
    "# print(data_)\n",
    "# TODO 指标\n",
    "y_true = data['标签']  # 这个地方从excel中提取\n",
    "\n",
    "# TODO 进行分词\n",
    "\n",
    "#  分词以及去除停用词\n",
    "def new_cut(data):\n",
    "    data = data.apply(lambda x: ' '.join(jieba.lcut(x)))\n",
    "    with open(stopwords_path, 'r', encoding='utf-8') as f:\n",
    "        stop_words = [line.strip() for line in f.readlines()]\n",
    "    # stop_words += (['\\n', ' ', '_x000D_', '摘要', '进行', '进行了', '操作', 'bug', '出现', '\\\\', 'n'])\n",
    "    stop_words.append('\\n')\n",
    "    stop_words.append(' ')\n",
    "    stop_words.append('_x000D_')\n",
    "    # stop_words.append('摘要')\n",
    "    sentences = []\n",
    "    for i in range(len(data)):\n",
    "        word = list(jieba.cut(data[i]))\n",
    "        word = [word1 for word1 in word if not word1 in stop_words]\n",
    "        # data[i] = ' '.join(word)\n",
    "        sentences.append(word)\n",
    "    return sentences\n",
    "\n",
    "data_1 = new_cut(data_)\n",
    "# print(data_1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# 读取bert的模型\n",
    "model = SentenceTransformer(\n",
    "    'LaBSE',  # 要使用的预训练模型\n",
    "    cache_folder=r\"./model\"  # 指定该模型在本地的缓存路径\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# sentence_bert\n",
    "def bert_exr(data):\n",
    "    sentences = []\n",
    "    for i in range(len(data)):\n",
    "        sentences.append(' '.join(data[i]))\n",
    "\n",
    "    features = model.encode(sentences)\n",
    "    features = sp.sparse.csr_matrix(features)\n",
    "    return features\n",
    "\n",
    "ver_sb = bert_exr(data_1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================HDB-Sentence-Bert==============\n",
      "======================纯度======================\n",
      "Purity HDBSCAN Clusters (sb): 0.5561959654178674\n",
      "====================调整后互信息====================\n",
      "Adjusted Mutual Info Score HDBSCAN Clusters (sb): 0.4606449282882575\n",
      "=====================V-measure====================\n",
      "V-measure Score HDBSCAN Clusters (sb): 0.6316505944578188\n"
     ]
    }
   ],
   "source": [
    "# HDBSCAN\n",
    "def hdb_cluster(data):\n",
    "    clustering = HDBSCAN(min_cluster_size=2, min_samples=1,\n",
    "                         cluster_selection_epsilon=0.71,alpha=1,cluster_selection_method='eom',metric='euclidean',\n",
    "                         allow_single_cluster=False,\n",
    "                         ).fit(data)\n",
    "    return clustering\n",
    "hdb_cluster_sb = hdb_cluster(ver_sb)\n",
    "def output(y_true,hdb_cluster_sb):\n",
    "    # 纯度\n",
    "    print('================HDB-Sentence-Bert==============')\n",
    "    print('======================纯度======================')\n",
    "    purity_hdb_cluster_sb = purity_score(y_true, hdb_cluster_sb.labels_)\n",
    "    print(\"Purity HDBSCAN Clusters (sb):\", purity_hdb_cluster_sb)\n",
    "    # 调整互信息\n",
    "    ami_hdb_cluster_sb = adjusted_mutual_info_score(y_true, hdb_cluster_sb.labels_)\n",
    "    print('====================调整后互信息====================')\n",
    "    print(\"Adjusted Mutual Info Score HDBSCAN Clusters (sb):\", ami_hdb_cluster_sb)\n",
    "    # V-measure\n",
    "    v_measure_hdb_cluster_sb = v_measure_score(y_true, hdb_cluster_sb.labels_)\n",
    "    print('=====================V-measure====================')\n",
    "    print(\"V-measure Score HDBSCAN Clusters (sb):\", v_measure_hdb_cluster_sb)\n",
    "\n",
    "output(y_true,hdb_cluster_sb)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
