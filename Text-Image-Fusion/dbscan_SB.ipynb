{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "from sklearn.base import BaseEstimator, ClusterMixin\n",
    "import jieba\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from sklearn.cluster import DBSCAN, AgglomerativeClustering, HDBSCAN\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score\n",
    "import spacy\n",
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "\n",
    "from docx import Document\n",
    "\n",
    "import config\n",
    "from sentence_transformers import util, SentenceTransformer\n",
    "from sklearn.metrics import adjusted_mutual_info_score, v_measure_score\n",
    "\n",
    "import openpyxl\n",
    "\n",
    "stopwords_path = './stopwords-master/hit_stopwords.txt'\n",
    "\n",
    "def cos_(ver,data_1):\n",
    "    data_1 = data_1.tolist()\n",
    "    cos_sims = []\n",
    "    ver = ver.toarray()\n",
    "    for j in range(len(ver)):\n",
    "        rows = []\n",
    "        for i in range(len(ver)):\n",
    "            sim = util.cos_sim(ver[j],ver[i])\n",
    "            rows.append(sim.tolist()[0][0])\n",
    "        cos_sims.append(rows)\n",
    "\n",
    "\n",
    "    workbook = openpyxl.Workbook()\n",
    "    worksheet = workbook.active\n",
    "\n",
    "    worksheet.append(['']+data_1)\n",
    "    for data,val in zip(data_1,cos_sims):\n",
    "        worksheet.append([data]+val)\n",
    "\n",
    "    workbook.save(config.next_feature_ext.ext+\".xlsx\")\n",
    "    workbook.close()\n",
    "\n",
    "    return sp.sparse.csr_matrix(cos_sims)\n",
    "\n",
    "def npeMethod(text):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "\n",
    "def data_post(data, cluster, file_name):\n",
    "    data['labels_'] = cluster.labels_\n",
    "\n",
    "\n",
    "    json_data = {}\n",
    "    for i in range(max(cluster.labels_) + 1):\n",
    "        json_data[f\"Cluster {i + 1}\"] = list(data[data['labels_'] == i]['discription'])\n",
    "    json_data[f\"Noise:\"] = list(data[data['labels_'] == -1]['discription'])\n",
    "\n",
    "    # file_name = '' + next_feature_ext + '_' + next_cluster_me + '.json'\n",
    "    with open('./result/' + file_name+\".json\", 'w', encoding='utf-8') as f:\n",
    "        f.write(json.dumps(json_data, ensure_ascii=False, indent=2))\n",
    "    # json.dump(json_data,f)\n",
    "\n",
    "def purity_score(y_true, y_pred):\n",
    "\n",
    "    cont_matrix = contingency_matrix(y_true, y_pred)\n",
    "\n",
    "\n",
    "    return np.sum(np.amax(cont_matrix, axis=0)) / np.sum(cont_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def read_data():\n",
    "    data_path = \"JayMe标注.xlsx\"\n",
    "    data = pd.read_excel(data_path, sheet_name=\"Sheet1\")  # , sep=',')\n",
    "    data_ = data[\"缺陷描述\"]\n",
    "    # print(data_)\n",
    "\n",
    "    y_true = data['标签']\n",
    "\n",
    "\n",
    "\n",
    "    def new_cut(data):\n",
    "        data = data.apply(lambda x: ' '.join(jieba.lcut(x)))\n",
    "        with open(stopwords_path, 'r', encoding='utf-8') as f:\n",
    "            stop_words = [line.strip() for line in f.readlines()]\n",
    "        stop_words.append('\\n')\n",
    "        stop_words.append(' ')\n",
    "        stop_words.append('_x000D_')\n",
    "        sentences = []\n",
    "        for i in range(len(data)):\n",
    "            word = list(jieba.cut(data[i]))\n",
    "            word = [word1 for word1 in word if not word1 in stop_words]\n",
    "            # data[i] = ' '.join(word)\n",
    "            sentences.append(word)\n",
    "        return sentences\n",
    "\n",
    "    data_1 = new_cut(data_)\n",
    "    return data_1,y_true\n",
    "\n",
    "data_1,y_true = read_data()\n",
    "# print(data_1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "model = SentenceTransformer(\n",
    "    'LaBSE',\n",
    "    cache_folder=r\"./model\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# sentence_bert\n",
    "def bert_exr(data):\n",
    "    sentences = []\n",
    "    for i in range(len(data)):\n",
    "        sentences.append(' '.join(data[i]))\n",
    "\n",
    "    features = model.encode(sentences)\n",
    "    features = sp.sparse.csr_matrix(features)\n",
    "    return features\n",
    "\n",
    "ver_sb = bert_exr(data_1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# DBSCAN\n",
    "def db_cluster(data):\n",
    "    clustering = DBSCAN(\n",
    "        eps=0.71,\n",
    "        min_samples=1,\n",
    "        metric='euclidean',\n",
    "        metric_params=config.dbscan.metric_params,\n",
    "        algorithm='auto',\n",
    "        leaf_size=config.dbscan.leaf_size,\n",
    "        p=config.dbscan.p,\n",
    "        n_jobs=config.dbscan.n_jobs,\n",
    "    ).fit(data)\n",
    "    # clustering = DBSCAN(eps=0.85, min_samples=2).fit(data)\n",
    "    # clustering = DBSCAN(eps=0.85, min_samples=2).fit(data)\n",
    "    return clustering\n",
    "db_cluster_sb = db_cluster(ver_sb)\n",
    "def output(y_true,db_cluster_sb):\n",
    "\n",
    "    purity_db_cluster_sb = purity_score(y_true, db_cluster_sb.labels_)\n",
    "    print(\"Purity DBSCAN Clusters (sb):\", purity_db_cluster_sb)\n",
    "\n",
    "    ami_db_cluster_sb = adjusted_mutual_info_score(y_true, db_cluster_sb.labels_)\n",
    "\n",
    "    print(\"Adjusted Mutual Info Score DBSCAN Clusters (sb):\", ami_db_cluster_sb)\n",
    "\n",
    "    v_measure_db_cluster_sb = v_measure_score(y_true, db_cluster_sb.labels_)\n",
    "\n",
    "    print(\"V-measure Score DBSCAN Clusters (sb):\", v_measure_db_cluster_sb)\n",
    "\n",
    "output(y_true,db_cluster_sb)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
