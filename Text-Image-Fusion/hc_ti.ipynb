{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "from sklearn.base import BaseEstimator, ClusterMixin\n",
    "import jieba\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from sklearn.cluster import DBSCAN, AgglomerativeClustering, HDBSCAN\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score\n",
    "import spacy\n",
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "\n",
    "from docx import Document\n",
    "\n",
    "import config\n",
    "from sentence_transformers import util, SentenceTransformer\n",
    "from sklearn.metrics import adjusted_mutual_info_score, v_measure_score\n",
    "\n",
    "import openpyxl\n",
    "\n",
    "stopwords_path = './stopwords-master/hit_stopwords.txt'\n",
    "\n",
    "def cos_(ver,data_1):\n",
    "    data_1 = data_1.tolist()\n",
    "    cos_sims = []\n",
    "    ver = ver.toarray()\n",
    "    for j in range(len(ver)):\n",
    "        rows = []\n",
    "        for i in range(len(ver)):\n",
    "            sim = util.cos_sim(ver[j],ver[i])\n",
    "            rows.append(sim.tolist()[0][0])\n",
    "        cos_sims.append(rows)\n",
    "\n",
    "    #存储成Excel\n",
    "    workbook = openpyxl.Workbook()\n",
    "    worksheet = workbook.active\n",
    "\n",
    "    worksheet.append(['']+data_1)\n",
    "    for data,val in zip(data_1,cos_sims):\n",
    "        worksheet.append([data]+val)\n",
    "\n",
    "    workbook.save(config.next_feature_ext.ext+\".xlsx\")\n",
    "    workbook.close()\n",
    "\n",
    "    return sp.sparse.csr_matrix(cos_sims)\n",
    "\n",
    "def npeMethod(text):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "\n",
    "def data_post(data, cluster, file_name):\n",
    "    data['labels_'] = cluster.labels_\n",
    "\n",
    "    # 存储为json\n",
    "    json_data = {}\n",
    "    for i in range(max(cluster.labels_) + 1):\n",
    "        json_data[f\"Cluster {i + 1}\"] = list(data[data['labels_'] == i]['discription'])\n",
    "    json_data[f\"Noise:\"] = list(data[data['labels_'] == -1]['discription'])\n",
    "\n",
    "    # file_name = '' + next_feature_ext + '_' + next_cluster_me + '.json'\n",
    "    with open('./result/' + file_name+\".json\", 'w', encoding='utf-8') as f:\n",
    "        f.write(json.dumps(json_data, ensure_ascii=False, indent=2))\n",
    "    # json.dump(json_data,f)\n",
    "\n",
    "def purity_score(y_true, y_pred):\n",
    "    # 计算纯度\n",
    "    # 使用混淆矩阵（contingency matrix），其中每一列代表一个真实类别，每一行代表一个聚类\n",
    "    cont_matrix = contingency_matrix(y_true, y_pred)\n",
    "\n",
    "    # 对于每个聚类（行），找出数量最多的真实类别（列）的数量\n",
    "    # 然后将这些数量加起来\n",
    "    # 最后，将总数除以样本总数以计算纯度\n",
    "    return np.sum(np.amax(cont_matrix, axis=0)) / np.sum(cont_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\17143\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.366 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "# 读取数据\n",
    "def read_data():\n",
    "    data_path = \"JayMe标注.xlsx\"\n",
    "    data = pd.read_excel(data_path, sheet_name=\"Sheet1\")  # , sep=',')\n",
    "    data_ = data[\"缺陷描述\"]\n",
    "    # print(data_)\n",
    "    # TODO 指标\n",
    "    y_true = data['标签']  # 这个地方从excel中提取\n",
    "\n",
    "    # TODO 进行分词\n",
    "\n",
    "    #  分词以及去除停用词\n",
    "    def new_cut(data):\n",
    "        data = data.apply(lambda x: ' '.join(jieba.lcut(x)))\n",
    "        with open(stopwords_path, 'r', encoding='utf-8') as f:\n",
    "            stop_words = [line.strip() for line in f.readlines()]\n",
    "        # stop_words += (['\\n', ' ', '_x000D_', '摘要', '进行', '进行了', '操作', 'bug', '出现', '\\\\', 'n'])\n",
    "        stop_words.append('\\n')\n",
    "        stop_words.append(' ')\n",
    "        stop_words.append('_x000D_')\n",
    "        # stop_words.append('摘要')\n",
    "        sentences = []\n",
    "        for i in range(len(data)):\n",
    "            word = list(jieba.cut(data[i]))\n",
    "            word = [word1 for word1 in word if not word1 in stop_words]\n",
    "            # data[i] = ' '.join(word)\n",
    "            sentences.append(word)\n",
    "        return sentences\n",
    "\n",
    "    data_1 = new_cut(data_)\n",
    "    return data_1,y_true\n",
    "\n",
    "data_1,y_true = read_data()\n",
    "# print(data_1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================HC-ti_idf======================\n",
      "======================纯度======================\n",
      "Purity HC Clusters (ti_idf): 0.579250720461095\n",
      "====================调整后互信息====================\n",
      "Adjusted Mutual Info Score HC Clusters (ti_idf): 0.3337962768550723\n",
      "=====================V-measure====================\n",
      "V-measure Score HC Clusters (ti_idf): 0.6138038567665091\n"
     ]
    }
   ],
   "source": [
    "def tfdif(data):\n",
    "    sentences = []\n",
    "    for i in range(len(data)):\n",
    "        sentences.append(' '.join(data[i]))\n",
    "    vectorizer_word = TfidfVectorizer(\n",
    "        input=config.tfdif.input,\n",
    "        encoding=config.tfdif.encoding,\n",
    "        decode_error='strict',\n",
    "        strip_accents=config.tfdif.strip_accents,\n",
    "        lowercase=config.tfdif.lowercase,\n",
    "        preprocessor=config.tfdif.preprocessor,\n",
    "        tokenizer=config.tfdif.tokenizer,\n",
    "        analyzer=\"word\",\n",
    "        stop_words=config.tfdif.stop_words,\n",
    "        token_pattern=config.tfdif.token_pattern,\n",
    "        ngram_range=(1,2),\n",
    "        max_df=0.9,\n",
    "        min_df=16,\n",
    "        max_features=5000,\n",
    "        vocabulary=config.tfdif.vocabulary,\n",
    "        binary=config.tfdif.binary,\n",
    "        dtype=config.tfdif.dtype,\n",
    "        norm=\"l2\",\n",
    "        use_idf=False,\n",
    "        smooth_idf=False,\n",
    "        sublinear_tf=False,\n",
    "    )\n",
    "    # vectorizer_word = TfidfVectorizer(\n",
    "    #     max_features=800000,\n",
    "    #                                   token_pattern=r\"(?u)\\b\\w+\\b\",\n",
    "    #                                   min_df=5,\n",
    "    #                                   # max_df=0.1,\n",
    "    #                                   analyzer='word',\n",
    "    #                                   ngram_range=(1, 2)\n",
    "    #                                   )\n",
    "    vectorizer_word = vectorizer_word.fit(sentences)\n",
    "    # print(type(data))\n",
    "    # print(vectorizer_word)\n",
    "    tfidf_matrix = vectorizer_word.transform(sentences)\n",
    "    return tfidf_matrix\n",
    "ver_ti_idf = tfdif(data_1)\n",
    "# HC\n",
    "def hc_cluster(data, n):\n",
    "    hc = AgglomerativeClustering(\n",
    "        n_clusters=n,\n",
    "        affinity=\"deprecated\",  # TODO(1.4): Remove\n",
    "        metric=config.hc.metric,  # TODO(1.4): Set to \"euclidean\"\n",
    "        memory=config.hc.memory,\n",
    "        connectivity=config.hc.connectivity,\n",
    "        compute_full_tree=config.hc.compute_full_tree,\n",
    "        linkage=config.hc.linkage,\n",
    "        distance_threshold=1.4,\n",
    "        compute_distances=config.hc.compute_distances,\n",
    "    ).fit(data.toarray())\n",
    "    # hc = AgglomerativeClustering(n_clusters=n, linkage='average').fit(data.toarray())\n",
    "    return hc\n",
    "\n",
    "n = None # 聚类的个数，自己设定一下\n",
    "hc_cluster_ti_idf = hc_cluster(ver_ti_idf,n)\n",
    "def output(y_true,hc_cluster_ti_idf):\n",
    "    # 纯度\n",
    "    print('======================HC-ti_idf======================')\n",
    "    print('======================纯度======================')\n",
    "    purity_hc_cluster_ti_idf = purity_score(y_true, hc_cluster_ti_idf.labels_)\n",
    "    print(\"Purity HC Clusters (ti_idf):\", purity_hc_cluster_ti_idf)\n",
    "    # 调整互信息\n",
    "    ami_hc_cluster_ti_idf = adjusted_mutual_info_score(y_true, hc_cluster_ti_idf.labels_)\n",
    "    print('====================调整后互信息====================')\n",
    "    print(\"Adjusted Mutual Info Score HC Clusters (ti_idf):\", ami_hc_cluster_ti_idf)\n",
    "    # V-measure\n",
    "    v_measure_hc_cluster_ti_idf = v_measure_score(y_true, hc_cluster_ti_idf.labels_)\n",
    "    print('=====================V-measure====================')\n",
    "    print(\"V-measure Score HC Clusters (ti_idf):\", v_measure_hc_cluster_ti_idf)\n",
    "\n",
    "output(y_true,hc_cluster_ti_idf)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
