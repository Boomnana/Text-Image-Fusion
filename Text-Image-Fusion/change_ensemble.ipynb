{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "from sklearn.base import BaseEstimator, ClusterMixin\n",
    "import jieba\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from sklearn.cluster import DBSCAN, AgglomerativeClustering, HDBSCAN\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score\n",
    "import spacy\n",
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "\n",
    "from docx import Document\n",
    "\n",
    "import config\n",
    "from sentence_transformers import util, SentenceTransformer\n",
    "from sklearn.metrics import adjusted_mutual_info_score, v_measure_score\n",
    "\n",
    "import openpyxl\n",
    "\n",
    "stopwords_path = './stopwords-master/hit_stopwords.txt'\n",
    "\n",
    "def cos_(ver,data_1):\n",
    "    data_1 = data_1.tolist()\n",
    "    cos_sims = []\n",
    "    ver = ver.toarray()\n",
    "    for j in range(len(ver)):\n",
    "        rows = []\n",
    "        for i in range(len(ver)):\n",
    "            sim = util.cos_sim(ver[j],ver[i])\n",
    "            rows.append(sim.tolist()[0][0])\n",
    "        cos_sims.append(rows)\n",
    "\n",
    "\n",
    "    workbook = openpyxl.Workbook()\n",
    "    worksheet = workbook.active\n",
    "\n",
    "    worksheet.append(['']+data_1)\n",
    "    for data,val in zip(data_1,cos_sims):\n",
    "        worksheet.append([data]+val)\n",
    "\n",
    "    workbook.save(config.next_feature_ext.ext+\".xlsx\")\n",
    "    workbook.close()\n",
    "\n",
    "    return sp.sparse.csr_matrix(cos_sims)\n",
    "\n",
    "def npeMethod(text):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "\n",
    "def data_post(data, cluster, file_name):\n",
    "    data['labels_'] = cluster.labels_\n",
    "\n",
    "\n",
    "    json_data = {}\n",
    "    for i in range(max(cluster.labels_) + 1):\n",
    "        json_data[f\"Cluster {i + 1}\"] = list(data[data['labels_'] == i]['discription'])\n",
    "    json_data[f\"Noise:\"] = list(data[data['labels_'] == -1]['discription'])\n",
    "\n",
    "    # file_name = '' + next_feature_ext + '_' + next_cluster_me + '.json'\n",
    "    with open('./result/' + file_name+\".json\", 'w', encoding='utf-8') as f:\n",
    "        f.write(json.dumps(json_data, ensure_ascii=False, indent=2))\n",
    "    # json.dump(json_data,f)\n",
    "\n",
    "def purity_score(y_true, y_pred):\n",
    "\n",
    "    cont_matrix = contingency_matrix(y_true, y_pred)\n",
    "\n",
    "\n",
    "    return np.sum(np.amax(cont_matrix, axis=0)) / np.sum(cont_matrix)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-22T12:56:28.569799900Z",
     "start_time": "2024-09-22T12:56:28.542031700Z"
    }
   },
   "id": "9c0f203331c2c717"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "data_path = \"JayMe标注.xlsx\"\n",
    "data = pd.read_excel(data_path, sheet_name=\"Sheet1\")  # , sep=',')\n",
    "data_ = data[\"缺陷描述\"]\n",
    "# print(data_)\n",
    "\n",
    "y_true = data['标签']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-22T12:56:31.879122900Z",
     "start_time": "2024-09-22T12:56:28.548804600Z"
    }
   },
   "id": "2270bab139782e14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def new_cut(data):\n",
    "    data = data.apply(lambda x: ' '.join(jieba.lcut(x)))\n",
    "    with open(stopwords_path, 'r', encoding='utf-8') as f:\n",
    "        stop_words = [line.strip() for line in f.readlines()]\n",
    "    stop_words.append('\\n')\n",
    "    stop_words.append(' ')\n",
    "    stop_words.append('_x000D_')\n",
    "    sentences = []\n",
    "    for i in range(len(data)):\n",
    "        word = list(jieba.cut(data[i]))\n",
    "        word = [word1 for word1 in word if not word1 in stop_words]\n",
    "        # data[i] = ' '.join(word)\n",
    "        sentences.append(word)\n",
    "    return sentences\n",
    "\n",
    "data_1 = new_cut(data_)\n",
    "# print(data_1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-22T13:03:19.183297200Z",
     "start_time": "2024-09-22T13:03:19.156090500Z"
    }
   },
   "id": "5b7d0c937a1b21d2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "model = SentenceTransformer(\n",
    "    'LaBSE',\n",
    "    cache_folder=r\"./model\"\n",
    ")\n",
    "# sentence_bert\n",
    "def bert_exr(data):\n",
    "    sentences = []\n",
    "    for i in range(len(data)):\n",
    "        sentences.append(' '.join(data[i]))\n",
    "\n",
    "    features = model.encode(sentences)\n",
    "    features = sp.sparse.csr_matrix(features)\n",
    "    return features\n",
    "\n",
    "ver_sb = bert_exr(data_1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def hdb_cluster(data):\n",
    "    clustering = HDBSCAN(min_cluster_size=2, min_samples=1,\n",
    "                         cluster_selection_epsilon=0.71,alpha=1,cluster_selection_method='eom',metric='euclidean',\n",
    "                         allow_single_cluster=False,\n",
    "                         ).fit(data)\n",
    "    return clustering\n",
    "hdb_cluster_sb = hdb_cluster(ver_sb)\n",
    "\n",
    "\n",
    "purity_hdb_cluster_sb = purity_score(y_true, hdb_cluster_sb.labels_)\n",
    "print(\"Purity HDBSCAN Clusters (sb):\", purity_hdb_cluster_sb)\n",
    "\n",
    "ami_hdb_cluster_sb = adjusted_mutual_info_score(y_true, hdb_cluster_sb.labels_)\n",
    "\n",
    "print(\"Adjusted Mutual Info Score HDBSCAN Clusters (sb):\", ami_hdb_cluster_sb)\n",
    "\n",
    "v_measure_hdb_cluster_sb = v_measure_score(y_true, hdb_cluster_sb.labels_)\n",
    "\n",
    "print(\"V-measure Score HDBSCAN Clusters (sb):\", v_measure_hdb_cluster_sb)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-22T12:56:35.074824200Z",
     "start_time": "2024-09-22T12:56:31.925947900Z"
    }
   },
   "id": "7ca904f9bdca093f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ti-idf\n",
    "def tfdif(data):\n",
    "    sentences = []\n",
    "    for i in range(len(data)):\n",
    "        sentences.append(' '.join(data[i]))\n",
    "    vectorizer_word = TfidfVectorizer(\n",
    "        input=config.tfdif.input,\n",
    "        encoding=config.tfdif.encoding,\n",
    "        decode_error='strict',\n",
    "        strip_accents=config.tfdif.strip_accents,\n",
    "        lowercase=config.tfdif.lowercase,\n",
    "        preprocessor=config.tfdif.preprocessor,\n",
    "        tokenizer=config.tfdif.tokenizer,\n",
    "        analyzer=\"word\",\n",
    "        stop_words=config.tfdif.stop_words,\n",
    "        token_pattern=config.tfdif.token_pattern,\n",
    "        ngram_range=(1,2),\n",
    "        max_df=0.9,\n",
    "        min_df=9,\n",
    "        max_features=5000,\n",
    "        vocabulary=config.tfdif.vocabulary,\n",
    "        binary=config.tfdif.binary,\n",
    "        dtype=config.tfdif.dtype,\n",
    "        norm=\"l2\",\n",
    "        use_idf=False,\n",
    "        smooth_idf=False,\n",
    "        sublinear_tf=False,\n",
    "    )\n",
    "    # vectorizer_word = TfidfVectorizer(\n",
    "    #     max_features=800000,\n",
    "    #                                   token_pattern=r\"(?u)\\b\\w+\\b\",\n",
    "    #                                   min_df=5,\n",
    "    #                                   # max_df=0.1,\n",
    "    #                                   analyzer='word',\n",
    "    #                                   ngram_range=(1, 2)\n",
    "    #                                   )\n",
    "    vectorizer_word = vectorizer_word.fit(sentences)\n",
    "    # print(type(data))\n",
    "    # print(vectorizer_word)\n",
    "    tfidf_matrix = vectorizer_word.transform(sentences)\n",
    "    return tfidf_matrix\n",
    "\n",
    "ver_ti_idf = tfdif(data_1)\n",
    "# HDBSCAN\n",
    "def hdb_cluster(data):\n",
    "    clustering = HDBSCAN(min_cluster_size=2, min_samples=1,\n",
    "                         cluster_selection_epsilon=0.75,alpha=1,cluster_selection_method='eom',metric='euclidean',\n",
    "                         allow_single_cluster=False,\n",
    "                         ).fit(data)\n",
    "    return clustering\n",
    "hdb_cluster_ti_idf = hdb_cluster(ver_ti_idf)\n",
    "\n",
    "purity_hdb_cluster_ti_idf = purity_score(y_true, hdb_cluster_ti_idf.labels_)\n",
    "print(\"Purity HDBSCAN Clusters (ti_idf):\", purity_hdb_cluster_ti_idf)\n",
    "print(\"Purity HDBSCAN Clusters (sb):\", purity_hdb_cluster_sb)\n",
    "\n",
    "ami_hdb_cluster_ti_idf = adjusted_mutual_info_score(y_true, hdb_cluster_ti_idf.labels_)\n",
    "ami_hdb_cluster_sb = adjusted_mutual_info_score(y_true, hdb_cluster_sb.labels_)\n",
    "\n",
    "print(\"Adjusted Mutual Info Score HDBSCAN Clusters (ti_idf):\", ami_hdb_cluster_ti_idf)\n",
    "print(\"Adjusted Mutual Info Score HDBSCAN Clusters (sb):\", ami_hdb_cluster_sb)\n",
    "\n",
    "v_measure_hdb_cluster_ti_idf = v_measure_score(y_true, hdb_cluster_ti_idf.labels_)\n",
    "v_measure_hdb_cluster_sb = v_measure_score(y_true, hdb_cluster_sb.labels_)\n",
    "\n",
    "print(\"V-measure Score HDBSCAN Clusters (ti_idf):\", v_measure_hdb_cluster_ti_idf)\n",
    "print(\"V-measure Score HDBSCAN Clusters (sb):\", v_measure_hdb_cluster_sb)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-22T13:46:32.674849300Z",
     "start_time": "2024-09-22T13:46:32.653510400Z"
    }
   },
   "id": "68c2fc60798cf350"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "model = SentenceTransformer(\n",
    "    'paraphrase-multilingual-mpnet-base-v2',\n",
    "    cache_folder=r\"./model\"\n",
    ")\n",
    "# sentence_bert\n",
    "def bert_exr(data):\n",
    "    sentences = []\n",
    "    for i in range(len(data)):\n",
    "        sentences.append(' '.join(data[i]))\n",
    "\n",
    "    features = model.encode(sentences)\n",
    "    features = sp.sparse.csr_matrix(features)\n",
    "    return features\n",
    "\n",
    "def db_cluster(data):\n",
    "    clustering = DBSCAN(\n",
    "        eps=1.3,\n",
    "        min_samples=1,\n",
    "        metric='euclidean',\n",
    "        metric_params=config.dbscan.metric_params,\n",
    "        algorithm='auto',\n",
    "        leaf_size=config.dbscan.leaf_size,\n",
    "        p=config.dbscan.p,\n",
    "        n_jobs=config.dbscan.n_jobs,\n",
    "    ).fit(data)\n",
    "    # clustering = DBSCAN(eps=0.85, min_samples=2).fit(data)\n",
    "    # clustering = DBSCAN(eps=0.85, min_samples=2).fit(data)\n",
    "    return clustering\n",
    "ver_sb = bert_exr(data_1)\n",
    "db_cluster_sb = db_cluster(ver_sb)\n",
    "\n",
    "purity_db_cluster_sb = purity_score(y_true, db_cluster_sb.labels_)\n",
    "\n",
    "print(\"Purity DBSCAN Clusters (sb):\", purity_db_cluster_sb)\n",
    "\n",
    "ami_db_cluster_sb = adjusted_mutual_info_score(y_true, db_cluster_sb.labels_)\n",
    "\n",
    "print(\"Adjusted Mutual Info Score DBSCAN Clusters (sb):\", ami_db_cluster_sb)\n",
    "\n",
    "v_measure_db_cluster_sb = v_measure_score(y_true, db_cluster_sb.labels_)\n",
    "\n",
    "print(\"V-measure Score DBSCAN Clusters (sb):\", v_measure_db_cluster_sb)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ti-idf\n",
    "def tfdif(data):\n",
    "    sentences = []\n",
    "    for i in range(len(data)):\n",
    "        sentences.append(' '.join(data[i]))\n",
    "    vectorizer_word = TfidfVectorizer(\n",
    "        input=config.tfdif.input,\n",
    "        encoding=config.tfdif.encoding,\n",
    "        decode_error='strict',\n",
    "        strip_accents=config.tfdif.strip_accents,\n",
    "        lowercase=config.tfdif.lowercase,\n",
    "        preprocessor=config.tfdif.preprocessor,\n",
    "        tokenizer=config.tfdif.tokenizer,\n",
    "        analyzer=\"word\",\n",
    "        stop_words=config.tfdif.stop_words,\n",
    "        token_pattern=config.tfdif.token_pattern,\n",
    "        ngram_range=(1,2),\n",
    "        max_df=0.9,\n",
    "        min_df=9,\n",
    "        max_features=5000,\n",
    "        vocabulary=config.tfdif.vocabulary,\n",
    "        binary=config.tfdif.binary,\n",
    "        dtype=config.tfdif.dtype,\n",
    "        norm=\"l2\",\n",
    "        use_idf=False,\n",
    "        smooth_idf=False,\n",
    "        sublinear_tf=False,\n",
    "    )\n",
    "    # vectorizer_word = TfidfVectorizer(\n",
    "    #     max_features=800000,\n",
    "    #                                   token_pattern=r\"(?u)\\b\\w+\\b\",\n",
    "    #                                   min_df=5,\n",
    "    #                                   # max_df=0.1,\n",
    "    #                                   analyzer='word',\n",
    "    #                                   ngram_range=(1, 2)\n",
    "    #                                   )\n",
    "    vectorizer_word = vectorizer_word.fit(sentences)\n",
    "    # print(type(data))\n",
    "    # print(vectorizer_word)\n",
    "    tfidf_matrix = vectorizer_word.transform(sentences)\n",
    "    return tfidf_matrix\n",
    "# DBSCAN\n",
    "def db_cluster(data):\n",
    "    clustering = DBSCAN(\n",
    "        eps=1.3,\n",
    "        min_samples=1,\n",
    "        metric='euclidean',\n",
    "        metric_params=config.dbscan.metric_params,\n",
    "        algorithm='auto',\n",
    "        leaf_size=config.dbscan.leaf_size,\n",
    "        p=config.dbscan.p,\n",
    "        n_jobs=config.dbscan.n_jobs,\n",
    "    ).fit(data)\n",
    "    # clustering = DBSCAN(eps=0.85, min_samples=2).fit(data)\n",
    "    # clustering = DBSCAN(eps=0.85, min_samples=2).fit(data)\n",
    "    return clustering\n",
    "\n",
    "ver_ti_idf = tfdif(data_1)\n",
    "\n",
    "db_cluster_ti_idf = db_cluster(ver_ti_idf)\n",
    "\n",
    "purity_db_cluster_ti_idf = purity_score(y_true, db_cluster_ti_idf.labels_)\n",
    "\n",
    "print(\"Purity DBSCAN Clusters (ti_idf):\", purity_db_cluster_ti_idf)\n",
    "\n",
    "ami_db_cluster_ti_idf = adjusted_mutual_info_score(y_true, db_cluster_ti_idf.labels_)\n",
    "\n",
    "print(\"Adjusted Mutual Info Score DBSCAN Clusters (ti_idf):\", ami_db_cluster_ti_idf)\n",
    "\n",
    "v_measure_db_cluster_ti_idf = v_measure_score(y_true, db_cluster_ti_idf.labels_)\n",
    "\n",
    "print(\"V-measure Score DBSCAN Clusters (ti_idf):\", v_measure_db_cluster_ti_idf)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-22T13:46:56.624540500Z",
     "start_time": "2024-09-22T13:46:53.706095200Z"
    }
   },
   "id": "61b6194d17c1c12b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# HC\n",
    "def hc_cluster(data, n):\n",
    "    hc = AgglomerativeClustering(\n",
    "        n_clusters=n,\n",
    "        affinity=\"deprecated\",  # TODO(1.4): Remove\n",
    "        metric=config.hc.metric,  # TODO(1.4): Set to \"euclidean\"\n",
    "        memory=config.hc.memory,\n",
    "        connectivity=config.hc.connectivity,\n",
    "        compute_full_tree=config.hc.compute_full_tree,\n",
    "        linkage=config.hc.linkage,\n",
    "        distance_threshold=2,\n",
    "        compute_distances=config.hc.compute_distances,\n",
    "    ).fit(data.toarray())\n",
    "    # hc = AgglomerativeClustering(n_clusters=n, linkage='average').fit(data.toarray())\n",
    "    return hc\n",
    "\n",
    "n = None\n",
    "hc_cluster_ti_idf = hc_cluster(ver_ti_idf,n)\n",
    "hc_cluster_sb = hc_cluster(ver_sb,n)\n",
    "\n",
    "purity_hc_cluster__ti_idf = purity_score(y_true, hc_cluster_ti_idf.labels_)\n",
    "purity_hc_cluster_sb = purity_score(y_true, hc_cluster_sb.labels_)\n",
    "\n",
    "print(\"Purity HC Clusters (ti_idf):\", purity_hc_cluster__ti_idf)\n",
    "print(\"Purity HC Clusters (sb):\", purity_hc_cluster_sb)\n",
    "\n",
    "ami_hc_cluster__ti_idf = adjusted_mutual_info_score(y_true, hc_cluster_ti_idf.labels_)\n",
    "ami_hc_cluster_sb = adjusted_mutual_info_score(y_true, hc_cluster_sb.labels_)\n",
    "\n",
    "print(\"Adjusted Mutual Info Score HC Clusters (ti_idf):\", ami_hc_cluster__ti_idf)\n",
    "print(\"Adjusted Mutual Info Score HC Clusters (sb):\", ami_hc_cluster_sb)\n",
    "\n",
    "v_measure_hc_cluster__ti_idf = v_measure_score(y_true, hc_cluster_ti_idf.labels_)\n",
    "v_measure_hc_cluster_sb = v_measure_score(y_true, hc_cluster_sb.labels_)\n",
    "\n",
    "print(\"V-measure Score HC Clusters (ti_idf):\", v_measure_hc_cluster__ti_idf)\n",
    "print(\"V-measure Score HC Clusters (sb):\", v_measure_hc_cluster_sb)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-22T14:17:40.896098700Z",
     "start_time": "2024-09-22T14:17:40.877674400Z"
    }
   },
   "id": "e125ab9a321dbb80"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class EnsembleCluster(BaseEstimator, ClusterMixin):\n",
    "    def __init__(self, estimators):\n",
    "        self.estimators = estimators\n",
    "\n",
    "    def fit_predict_mean(self, X):\n",
    "        cluster_probs = []\n",
    "        max_num_clusters = 0\n",
    "\n",
    "\n",
    "        for estimator in self.estimators:\n",
    "            if isinstance(estimator, AgglomerativeClustering):\n",
    "                labels = estimator.fit_predict(X.toarray())\n",
    "            else:\n",
    "                labels = estimator.fit_predict(X)\n",
    "            max_num_clusters = max(max_num_clusters, len(np.unique(labels)))\n",
    "\n",
    "\n",
    "        for estimator in self.estimators:\n",
    "            if isinstance(estimator, AgglomerativeClustering):\n",
    "                labels = estimator.fit_predict(X.toarray())\n",
    "            else:\n",
    "                labels = estimator.fit_predict(X)\n",
    "            unique_labels = np.unique(labels)\n",
    "            num_clusters = len(unique_labels)\n",
    "\n",
    "\n",
    "            probs = np.zeros((X.shape[0], max_num_clusters))\n",
    "\n",
    "\n",
    "            for i, label in enumerate(unique_labels):\n",
    "                if label != -1:\n",
    "                    probs[:, i] = (labels == label).astype(float)\n",
    "\n",
    "            cluster_probs.append(probs)\n",
    "\n",
    "\n",
    "        avg_probs = np.mean(cluster_probs, axis=0)\n",
    "\n",
    "\n",
    "        final_labels = np.argmax(avg_probs, axis=1)\n",
    "\n",
    "        return final_labels\n",
    "\n",
    "    def fit_predict_append(self, X):\n",
    "        cluster_labels = []\n",
    "        for estimator in self.estimators:\n",
    "            # print(type(estimator))\n",
    "            if type(estimator) is AgglomerativeClustering:\n",
    "                # labels = estimator.fit(X.toarray()).labels_\n",
    "                labels = estimator.fit_predict(X.toarray())\n",
    "            else:\n",
    "                # labels = estimator.fit(X).labels_\n",
    "                labels = estimator.fit_predict(X)\n",
    "            # print(type(labels))\n",
    "            cluster_labels.append(labels)\n",
    "\n",
    "        # Voting: Assign each data point to the cluster with the most votes\n",
    "        ensemble_labels = np.array(cluster_labels).T\n",
    "\n",
    "        final_labels = []\n",
    "        for row in ensemble_labels:\n",
    "            unique_labels, counts = np.unique(row, return_counts=True)\n",
    "            if len(unique_labels) == 1 and unique_labels[0] == -1:\n",
    "                # Handle samples with no cluster assignment\n",
    "                final_labels.append(-1)\n",
    "            else:\n",
    "                final_labels.append(unique_labels[np.argmax(counts)])\n",
    "\n",
    "        # final_labels = np.array([np.argmax(np.bincount(row)) for row in ensemble_labels])\n",
    "        # print('1')\n",
    "\n",
    "        return np.array(final_labels)\n",
    "\n",
    "hdbscan = HDBSCAN(min_cluster_size=2, min_samples=1)\n",
    "dbscan = DBSCAN(eps=0.03, min_samples=2)\n",
    "hc = AgglomerativeClustering(\n",
    "    n_clusters=n,\n",
    "    affinity=config.hc.affinity,  # TODO(1.4): Remove\n",
    "    metric=config.hc.metric,  # TODO(1.4): Set to \"euclidean\"\n",
    "    memory=config.hc.memory,\n",
    "    connectivity=config.hc.connectivity,\n",
    "    compute_full_tree=config.hc.compute_full_tree,\n",
    "    linkage=config.hc.linkage,\n",
    "    distance_threshold=config.hc.distance_threshold,\n",
    "    compute_distances=config.hc.compute_distances,\n",
    ")\n",
    "\n",
    "ensemble_cluster = EnsembleCluster(estimators=[hdbscan, dbscan, hc])\n",
    "#ensemble_cluster\n",
    "ensemble_cluster_append_ti_idf = ensemble_cluster.fit_predict_append(ver_ti_idf)\n",
    "ensemble_cluster_append_sb = ensemble_cluster.fit_predict_append(ver_sb)\n",
    "ensemble_cluster_mean_ti_idf = ensemble_cluster.fit_predict_mean(ver_ti_idf)\n",
    "ensemble_cluster_mean_sb = ensemble_cluster.fit_predict_mean(ver_sb)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-22T13:29:11.402814100Z",
     "start_time": "2024-09-22T13:29:03.014469100Z"
    }
   },
   "id": "1b22ad02bbff8ddc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "purity_ensemble_cluster_append__ti_idf = purity_score(y_true, ensemble_cluster_append_ti_idf)\n",
    "purity_ensemble_cluster_append_sb = purity_score(y_true, ensemble_cluster_append_sb)\n",
    "purity_ensemble_cluster_mean__ti_idf = purity_score(y_true, ensemble_cluster_mean_ti_idf)\n",
    "purity_ensemble_cluster_mean_sb = purity_score(y_true, ensemble_cluster_mean_sb)\n",
    "\n",
    "ami_ensemble_cluster_append__ti_idf = adjusted_mutual_info_score(y_true, ensemble_cluster_append_ti_idf)\n",
    "ami_ensemble_cluster_append_sb = adjusted_mutual_info_score(y_true, ensemble_cluster_append_sb)\n",
    "ami_ensemble_cluster_mean__ti_idf = adjusted_mutual_info_score(y_true, ensemble_cluster_mean_ti_idf)\n",
    "ami_ensemble_cluster_mean_sb = adjusted_mutual_info_score(y_true, ensemble_cluster_mean_sb)\n",
    "\n",
    "v_measure_ensemble_cluster_append__ti_idf = v_measure_score(y_true, ensemble_cluster_append_ti_idf)\n",
    "v_measure_ensemble_cluster_append_sb = v_measure_score(y_true, ensemble_cluster_append_sb)\n",
    "v_measure_ensemble_cluster_mean__ti_idf = v_measure_score(y_true, ensemble_cluster_mean_ti_idf)\n",
    "v_measure_ensemble_cluster_mean_sb = v_measure_score(y_true, ensemble_cluster_mean_sb)\n",
    "\n",
    "\n",
    "print(\"Purity HDBSCAN Clusters (ti_idf):\", purity_hdb_cluster_ti_idf)\n",
    "print(\"Purity HDBSCAN Clusters (sb):\", purity_hdb_cluster_sb)\n",
    "print(\"Purity DBSCAN Clusters (ti_idf):\", purity_db_cluster_ti_idf)\n",
    "print(\"Purity DBSCAN Clusters (sb):\", purity_db_cluster_sb)\n",
    "print(\"Purity HC Clusters (ti_idf):\", purity_hc_cluster__ti_idf)\n",
    "print(\"Purity HC Clusters (sb):\", purity_hc_cluster_sb)\n",
    "print(\"Purity ensemble Clusters append (ti_idf):\", purity_ensemble_cluster_append__ti_idf)\n",
    "print(\"Purity ensemble Clusters append (sb):\", purity_ensemble_cluster_append_sb)\n",
    "print(\"Purity ensemble Clusters mean (ti_idf):\", purity_ensemble_cluster_mean__ti_idf)\n",
    "print(\"Purity ensemble Clusters mean (sb):\", purity_ensemble_cluster_mean_sb)\n",
    "\n",
    "print(\"Adjusted Mutual Info Score HDBSCAN Clusters (ti_idf):\", ami_hdb_cluster_ti_idf)\n",
    "print(\"Adjusted Mutual Info Score HDBSCAN Clusters (sb):\", ami_hdb_cluster_sb)\n",
    "print(\"Adjusted Mutual Info Score DBSCAN Clusters (ti_idf):\", ami_db_cluster_ti_idf)\n",
    "print(\"Adjusted Mutual Info Score DBSCAN Clusters (sb):\", ami_db_cluster_sb)\n",
    "print(\"Adjusted Mutual Info Score HC Clusters (ti_idf):\", ami_hc_cluster__ti_idf)\n",
    "print(\"Adjusted Mutual Info Score HC Clusters (sb):\", ami_hc_cluster_sb)\n",
    "print(\"Adjusted Mutual Info Score ensemble Clusters append (ti_idf):\", ami_ensemble_cluster_append__ti_idf)\n",
    "print(\"Adjusted Mutual Info Score ensemble Clusters append (sb):\", ami_ensemble_cluster_append_sb)\n",
    "print(\"Adjusted Mutual Info Score ensemble Clusters mean (ti_idf):\", ami_ensemble_cluster_mean__ti_idf)\n",
    "print(\"Adjusted Mutual Info Score ensemble Clusters mean (sb):\", ami_ensemble_cluster_mean_sb)\n",
    "\n",
    "print(\"V-measure Score HDBSCAN Clusters (ti_idf):\", v_measure_hdb_cluster_ti_idf)\n",
    "print(\"V-measure Score HDBSCAN Clusters (sb):\", v_measure_hdb_cluster_sb)\n",
    "print(\"V-measure Score DBSCAN Clusters (ti_idf):\", v_measure_db_cluster_ti_idf)\n",
    "print(\"V-measure Score DBSCAN Clusters (sb):\", v_measure_db_cluster_sb)\n",
    "print(\"V-measure Score HC Clusters (ti_idf):\", v_measure_hc_cluster__ti_idf)\n",
    "print(\"V-measure Score HC Clusters (sb):\", v_measure_hc_cluster_sb)\n",
    "print(\"V-measure Score ensemble Clusters append (ti_idf):\", v_measure_ensemble_cluster_append__ti_idf)\n",
    "print(\"V-measure Score ensemble Clusters append (sb):\", v_measure_ensemble_cluster_append_sb)\n",
    "print(\"V-measure Score ensemble Clusters mean (ti_idf):\", v_measure_ensemble_cluster_mean__ti_idf)\n",
    "print(\"V-measure Score ensemble Clusters mean (sb):\", v_measure_ensemble_cluster_mean_sb)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-22T13:32:15.917080100Z",
     "start_time": "2024-09-22T13:32:15.887897100Z"
    }
   },
   "id": "205e62a24e845279"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
